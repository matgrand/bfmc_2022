{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_ahead.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_ahead.onnx\"\n",
    "max_load = 500_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "# #very good\n",
    "# class LaneKeeperAhead(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=16),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=16, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# lane_keeper_ahead = LaneKeeperAhead(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeperAhead(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.3\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 4, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 32, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.BatchNorm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*32, out_features=16),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=16, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper_ahead = LaneKeeperAhead(out_dim=1,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper_ahead.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper_ahead(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    # #add salt and pepper noise\n",
    "    # sp_noise = randint(0, 4, size=img.shape, dtype=np.uint8)\n",
    "    # sp_noise = np.where(sp_noise == 0, np.zeros_like(img), 255*np.ones_like(img))\n",
    "    # # img = cv.bitwise_xor(img, sp_noise)\n",
    "\n",
    "\n",
    "    # #reduce contrast\n",
    "    # const = np.random.uniform(0.1,0.8)\n",
    "    # # if np.random.uniform() > .5:\n",
    "    # #     const = const*0.2\n",
    "    # img = 127*(1-const) + img*const\n",
    "    # img = img.astype(np.uint8)\n",
    "\n",
    "    #add noise \n",
    "    std = 80\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    # #add random brightness\n",
    "    # max_brightness = 60\n",
    "    # brightness = randint(-max_brightness, max_brightness)\n",
    "    # if brightness > 0:\n",
    "    #     img = cv.add(img, brightness)\n",
    "    # elif brightness < 0:\n",
    "    #     img = cv.subtract(img, -brightness)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (randint(1,3),randint(1,3)))\n",
    "\n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(5000):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                # sample = [sample[0], sample[1], sample[2], sample[3], sample[4]] #e2=lateral error, e3=yaw error point ahead, curvature, dist stopline, angle stopline\n",
    "                sample = [sample[1]]\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    # cv.putText(img, f'{np.rad2deg(reg_label[0]):.1f}', (5,25), cv.FONT_HERSHEY_SIMPLEX, 0.3,255, 1)\n",
    "                    MAX_SHOW = 1000\n",
    "                    max_show = MAX_SHOW\n",
    "                    if i < max_show:\n",
    "                        cv.imshow('img', img)\n",
    "                        key = cv.waitKey(1)\n",
    "                        if i == max_show-1:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 140662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291820/291820 [13:32<00:00, 359.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([281324, 32, 32, 1])\n",
      "all images: torch.Size([151158, 32, 32, 1])\n",
      "self.data shape: 151158\n",
      "road_labels shape = 281324\n",
      "\n",
      "all imgs: torch.Size([432482, 32, 32, 1])\n",
      "data: (432482, 1)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 1])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    # err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        # err2 = output[:, 0]\n",
    "        err3 = output[:, 0]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        # err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 0]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        # err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + L1_loss + L2_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        # err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    # err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper_ahead.eval()\n",
    "    err_losses3 = []\n",
    "    # err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper_ahead(input)\n",
    "\n",
    "        regr_out = output\n",
    "        # err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 0]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        # err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 0]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "\n",
    "        # err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  155/500,  loss = MSELoss() \n",
      "yaw_err_loss3: 0.0345,   Val: 0.0393, best_val: 0.0335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 6/48 [00:03<00:22,  1.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_158230/1778378720.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mregr_loss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr_loss_fn1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mregr_loss_fn2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0merr_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mval_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_158230/42942214.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loop over the training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Move the input and target data to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_deep_learning_2021/dl_env/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_158230/1658561793.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.003 #0.005\n",
    "epochs = 500\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper_ahead.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn1 = nn.MSELoss() #before epochs/2\n",
    "regr_loss_fn2 = nn.MSELoss() #after epochs/2 for finetuning\n",
    "\n",
    "best_val = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        regr_loss_fn = regr_loss_fn1 if epoch < epochs//2 else regr_loss_fn2\n",
    "        err_loss3 = train_epoch(lane_keeper_ahead, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss3 = val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    if val_loss3 < best_val:\n",
    "        best_val = val_loss3\n",
    "        torch.save(lane_keeper_ahead.state_dict(), model_name)\n",
    "        print(\"model saved\")\n",
    "    \n",
    "    print(f\"Epoch  {epoch+1}/{epochs},  loss = {regr_loss_fn} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}, best_val: {best_val:.4f}\")\n",
    "    # print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improving randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 433/433 [00:01<00:00, 280.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaw_err3_loss: 0.03353552892804146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "lane_keeper_ahead.load_state_dict(torch.load(model_name))\n",
    "err_loss3 = val_epoch(lane_keeper_ahead, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "# print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(4, 4, 5, 5)\n",
      "(32, 4, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper_ahead.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 1, 4, 'conv0', size=(8,2))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 4, 4, 'conv1', size=(5,5)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 8, 8, 'conv2', size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeperAhead(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.3, inplace=False)\n",
       "    (11): Conv2d(4, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper_ahead.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper_ahead.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper_ahead.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper_ahead, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper_ahead.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6610.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[-0.09873553]]\n",
      "Predictions shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
