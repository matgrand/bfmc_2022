{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # this is another plotting library for interactive plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, manifold # we will use the metrics and manifold learning modules from scikit-learn\n",
    "from pathlib import Path # to interact with file paths\n",
    "from PIL import Image # to interact with images\n",
    "from tqdm import tqdm # progress bar\n",
    "from pprint import pprint # pretty print (useful for a more readable print of objects like lists or dictionaries)\n",
    "from IPython.display import clear_output # to clear the output of the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "num_channels = 1\n",
    "SIZE = (32,32)\n",
    "model_name = 'models/lane_keeper_small.pt'\n",
    "onnx_lane_keeper_path = \"models/lane_keeper_small.onnx\"\n",
    "max_load = 250_000 #note: it will be ~50% more since training points with pure road gets flipped with inverted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Net and create Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NETWORK ARCHITECTURE\n",
    "# #very good\n",
    "# class LaneKeeper(nn.Module):\n",
    "#     def __init__(self, out_dim=4, channels=1): \n",
    "#         super().__init__()\n",
    "#         ### Convoluational layers\n",
    "#         self.conv = nn.Sequential( #in = (SIZE)\n",
    "#             nn.Conv2d(channels, 8, kernel_size=5, stride=1), #out = 30\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2), #out=15\n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.Conv2d(8, 4, kernel_size=5, stride=1), #out = 12\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=1), #out=11\n",
    "#             # nn.BatchNorm2d(4),\n",
    "#             nn.Conv2d(4, 4, kernel_size=6, stride=1), #out = 6\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "#         self.flat = nn.Flatten()\n",
    "#         ### Linear sections\n",
    "#         self.lin = nn.Sequential(\n",
    "#             # First linear layer\n",
    "#             nn.Linear(in_features=4*4*4, out_features=16),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(in_features=16, out_features=out_dim),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.flat(x)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "\n",
    "class LaneKeeper(nn.Module):\n",
    "    def __init__(self, out_dim=4, channels=1): \n",
    "        super().__init__()\n",
    "        ### Convoluational layers\n",
    "        prob = 0.2\n",
    "        self.conv = nn.Sequential( #in = (SIZE)\n",
    "            nn.Conv2d(channels, 4, kernel_size=5, stride=1), #out = 28\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=14\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 4, kernel_size=5, stride=1), #out = 10\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #out=5\n",
    "            nn.Dropout(p=prob),\n",
    "            nn.Conv2d(4, 128, kernel_size=5, stride=1), #out = 1\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.flat = nn.Flatten()\n",
    "        ### Linear sections\n",
    "        self.lin = nn.Sequential(\n",
    "            #normalize\n",
    "            # nn.Batch\n",
    "            # Norm1d(3*3*4),\n",
    "            # First linear layer\n",
    "            nn.Linear(in_features=1*1*128, out_features=32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(in_features=32, out_features=out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "lane_keeper = LaneKeeper(out_dim=2,channels=num_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "out shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# TEST NET INPUTS/OUTPUTS\n",
    "#show the image with opencv\n",
    "img = cv.imread('tests/test_img.jpg')\n",
    "img = cv.resize(img, SIZE)\n",
    "if num_channels == 1:\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "#convert to tensor\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.permute(2,0,1)\n",
    "#add dimension\n",
    "img = img.unsqueeze(0).to(device)\n",
    "print(img.shape)\n",
    "\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = lane_keeper(img)\n",
    "    print(f'out shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 94595.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# IMG LOADER AND AUGMENTATION\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from time import time, sleep\n",
    "\n",
    "\n",
    "def load_and_augment_img(img, folder='training_imgs'):\n",
    "    #convert to gray\n",
    "    img = cv.resize(img, (4*SIZE[1], 4*SIZE[0]))\n",
    "\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    #create random ellipses to simulate light from the sun\n",
    "    light = np.zeros(img.shape, dtype=np.uint8)\n",
    "    #add ellipses\n",
    "    for j in range(2):\n",
    "        cent = (randint(0, img.shape[0]), randint(0, img.shape[1]))\n",
    "        axes_length = (randint(10//4, 50//4), randint(50//4, 300//4))\n",
    "        angle = randint(0, 360)\n",
    "        light = cv.ellipse(light, cent, axes_length, angle, 0, 360, 255, -1)\n",
    "    #create an image of random white and black pixels\n",
    "    light = cv.blur(light, (50,50))\n",
    "    noise = randint(0, 2, size=img.shape, dtype=np.uint8)*255\n",
    "    light = cv.subtract(light, noise)\n",
    "    light = np.clip(light, 0, 51)\n",
    "    light *= 5\n",
    "    #add light to the image\n",
    "    img = cv.add(img, light)\n",
    "\n",
    "    # cv.imshow('light', light)\n",
    "    # if cv.waitKey(0) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "    #blur the image\n",
    "    img = cv.blur(img, (randint(1, 5), randint(1, 5)))\n",
    "\n",
    "    # cut the top third of the image, let it 640x320\n",
    "    img = img[int(img.shape[0]/3):,:] ################################# /3\n",
    "    # assert img.shape == (320,640), f'img shape cut = {img.shape}'\n",
    "\n",
    "    #edges\n",
    "    img = cv.resize(img, (2*SIZE[1], 2*SIZE[0]))\n",
    "\n",
    "    r = randint(0, 5)\n",
    "    if r == 0:\n",
    "        #dilate\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.dilate(img, kernel, iterations=1)\n",
    "    elif r == 1:\n",
    "        #erode\n",
    "        kernel = np.ones((randint(1, 5), randint(1, 5)), np.uint8)\n",
    "        img = cv.erode(img, kernel, iterations=1)\n",
    "\n",
    "\n",
    "    #edges    \n",
    "    img = cv.Canny(img, 100, 200)\n",
    "\n",
    "    #blur\n",
    "    img = cv.blur(img, (3,3))\n",
    "\n",
    "    #resize \n",
    "    img = cv.resize(img, SIZE)\n",
    "\n",
    "    # #get max brightness\n",
    "    # max_brightness = np.max(img)\n",
    "    # ratio = 255.0/max_brightness\n",
    "    # #normalize\n",
    "    # img = (img*ratio).astype(np.uint8)\n",
    "\n",
    "    #add random tilt\n",
    "    max_offset = 3\n",
    "    offset = randint(-max_offset, max_offset)\n",
    "    img = np.roll(img, offset, axis=0)\n",
    "    if offset > 0:\n",
    "        img[:offset, :] = 0 #randint(0,255)\n",
    "    elif offset < 0:\n",
    "        img[offset:, :] = 0 # randint(0,255)\n",
    "    \n",
    "    # #add salt and pepper noise\n",
    "    # sp_noise = randint(0, 4, size=img.shape, dtype=np.uint8)\n",
    "    # sp_noise = np.where(sp_noise == 0, np.zeros_like(img), 255*np.ones_like(img))\n",
    "    # # img = cv.bitwise_xor(img, sp_noise)\n",
    "\n",
    "\n",
    "    # #reduce contrast\n",
    "    # const = np.random.uniform(0.1,0.8)\n",
    "    # # if np.random.uniform() > .5:\n",
    "    # #     const = const*0.2\n",
    "    # img = 127*(1-const) + img*const\n",
    "    # img = img.astype(np.uint8)\n",
    "\n",
    "    #add noise \n",
    "    std = 80\n",
    "    std = randint(1, std)\n",
    "    noisem = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.subtract(img, noisem)\n",
    "    noisep = randint(0, std, img.shape, dtype=np.uint8)\n",
    "    img = cv.add(img, noisep)\n",
    "\n",
    "    # #add random brightness\n",
    "    # max_brightness = 60\n",
    "    # brightness = randint(-max_brightness, max_brightness)\n",
    "    # if brightness > 0:\n",
    "    #     img = cv.add(img, brightness)\n",
    "    # elif brightness < 0:\n",
    "    #     img = cv.subtract(img, -brightness)\n",
    "\n",
    "    # #blur \n",
    "    # img = cv.blur(img, (randint(1,3),randint(1,3)))\n",
    "\n",
    "    # # invert color\n",
    "    # if np.random.uniform(0, 1) > 0.6:\n",
    "    #     img = cv.bitwise_not(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "# cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "\n",
    "for i in range(5000):\n",
    "    img = cv.imread(os.path.join('training_imgs', f'img_{i+1}.png'))\n",
    "    img = load_and_augment_img(img)\n",
    "    cv.imshow('img', img)z\n",
    "    key = cv.waitKey(100)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, max_load=1000, channels=3):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.data = []\n",
    "        self.channels = channels\n",
    "\n",
    "        #classification label for road ahead = 1,0,0,0,1,0,0,0,0,0,0,0,0,0,0\n",
    "        road_images_indexes = []\n",
    "        tot_lines = 0\n",
    "        with open(folder+'/classification_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            tot_lines = len(lines)\n",
    "            for i,line in enumerate(lines):\n",
    "                if line == '1,0,0,0,1,0,0,0,0,0,0,0,0,0,0':\n",
    "                    road_images_indexes.append(i)\n",
    "        print(f'total pure road images: {len(road_images_indexes)}')\n",
    "        road_imgs_mask = np.zeros(tot_lines, dtype=np.bool)\n",
    "        road_imgs_mask[road_images_indexes] = True\n",
    "\n",
    "        with open(folder+'/regression_labels.csv', 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            lines = lines[0:-1] #remove footer\n",
    "            # Get x and y values from each line and append to self.data\n",
    "            max_load = min(max_load, len(lines))\n",
    "            # self.all_imgs = torch.zeros((2*max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8) #adding flipped img\n",
    "            self.all_imgs = torch.zeros((max_load, SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "\n",
    "            # road images specifically are added again along with their flipped image and label\n",
    "            road_imgs = torch.zeros((2*len(road_images_indexes), SIZE[1], SIZE[0], channels), dtype=torch.uint8)\n",
    "            road_labels = []\n",
    "\n",
    "            cv.namedWindow('img', cv.WINDOW_NORMAL)\n",
    "            # cv.setWindowProperty('img', cv.WND_PROP_FULLSCREEN, cv.WINDOW_FULLSCREEN)\n",
    "            road_idx = 0\n",
    "            all_img_idx = 0\n",
    "            for i in tqdm(range(max_load)):\n",
    "\n",
    "                #label\n",
    "                line = lines[i]\n",
    "                sample = line.split(',')\n",
    "                #keep only info related to the lane, discard distance from stop line \n",
    "                sample = [sample[0], sample[1], sample[3]] #e2=lateral error, e3=yaw error point ahead, curvature\n",
    "                reg_label = np.array([float(s) for s in sample], dtype=np.float32)\n",
    "\n",
    "                #img \n",
    "                img = cv.imread(os.path.join(folder, f'img_{i+1}.png'))\n",
    "\n",
    "                #check if its in the road images\n",
    "                if road_imgs_mask[i]:\n",
    "                    img_r = load_and_augment_img(img.copy())\n",
    "                    # cv.imshow('imgR', img_r)\n",
    "                    img_r = img_r[:,:,np.newaxis]\n",
    "\n",
    "                    img_l = cv.flip(img, 1)\n",
    "                    img_l = load_and_augment_img(img_l)\n",
    "                    # cv.imshow('imgL', img_l)\n",
    "                    img_l = img_l[:,:,np.newaxis]\n",
    "                    # cv.waitKey(1)\n",
    "\n",
    "                    road_imgs[2*road_idx] = torch.from_numpy(img_r)\n",
    "                    road_imgs[2*road_idx+1] = torch.from_numpy(img_l)\n",
    "                    road_labels.append(reg_label)\n",
    "                    road_labels.append(-reg_label)\n",
    "                    road_idx += 1\n",
    "\n",
    "                else:\n",
    "                    img = load_and_augment_img(img)\n",
    "                    # cv.putText(img, f'{reg_label[0]*10.0}', (5,5), cv.FONT_HERSHEY_SIMPLEX, 0.4,255, 1)\n",
    "                    MAX_SHOW = 1000\n",
    "                    max_show = MAX_SHOW\n",
    "                    if i < max_show:\n",
    "                        cv.imshow('img', img)\n",
    "                        key = cv.waitKey(1)\n",
    "                        if i == max_show-1:\n",
    "                            cv.destroyAllWindows()\n",
    "                    #add a dimension to the image\n",
    "                    img = img[:, :,np.newaxis]\n",
    "                    self.all_imgs[all_img_idx] = torch.from_numpy(img)\n",
    "                    self.data.append(reg_label)\n",
    "                    all_img_idx += 1\n",
    "\n",
    "            #cut imgs to the right length\n",
    "            road_imgs = road_imgs[:2*road_idx]\n",
    "            self.all_imgs = self.all_imgs[:all_img_idx]\n",
    "\n",
    "            #concatenate all_imgs and road_imgs\n",
    "            print(f'road images: {road_imgs.shape}')\n",
    "            print(f'all images: {self.all_imgs.shape}')\n",
    "            self.all_imgs = torch.cat((self.all_imgs, road_imgs), dim=0)\n",
    "            print(f'self.data shape: {len(self.data)}')\n",
    "            print(f'road_labels shape = {len(road_labels)}')\n",
    "            self.data = np.concatenate((np.array(self.data), np.array(road_labels)), axis=0)\n",
    "\n",
    "            print(f'\\nall imgs: {self.all_imgs.shape}')\n",
    "            print(f'data: {self.data.shape}')\n",
    "\n",
    "            #free road_imgs from memory\n",
    "            del road_imgs\n",
    "            del road_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is simply the length of the self.data list\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # img = read_image(os.path.join(self.folder, f'img_{idx+1}.png'))\n",
    "        # img = img.float()\n",
    "        img = self.all_imgs[idx]\n",
    "        img = img.permute(2, 0, 1).float()\n",
    "        value = self.data[idx]\n",
    "        return img, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pure road images: 126213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245182/245182 [13:19<00:00, 306.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road images: torch.Size([252426, 32, 32, 1])\n",
      "all images: torch.Size([118969, 32, 32, 1])\n",
      "self.data shape: 118969\n",
      "road_labels shape = 252426\n",
      "\n",
      "all imgs: torch.Size([371395, 32, 32, 1])\n",
      "data: (371395, 3)\n"
     ]
    }
   ],
   "source": [
    "#create dataset #takes a long time but then training is faster\n",
    "train_dataset = CsvDataset('training_imgs', max_load=max_load, channels=num_channels)\n",
    "#split dataset into train and val\n",
    "train_size = int(0.9*len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 1, 32, 32])\n",
      "torch.Size([8192, 3])\n"
     ]
    }
   ],
   "source": [
    "#test dataloader\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_epoch(model, dataloader, regr_loss_fn, optimizer, L1_lambda=0.0, L2_lambda=0.0,  device=device):\n",
    "    # Set the model to training mode\n",
    "    model.train() #train\n",
    "    # Initialize the loss\n",
    "    err_losses2 = []\n",
    "    err_losses3 = []\n",
    "    # curv_losses = []\n",
    "\n",
    "    # Loop over the training batches\n",
    "    for (input, regr_label) in tqdm(dataloader):\n",
    "        # Move the input and target data to the selected device\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the output\n",
    "        output = model(input)\n",
    "\n",
    "        #regression\n",
    "        err2 = output[:, 0]\n",
    "        err3 = output[:, 1]\n",
    "        # curv_out = output[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        # Compute the losses\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "\n",
    "        #L1 regularization\n",
    "        L1_norm = sum(p.abs().sum() for p in model.conv.parameters())\n",
    "        L1_loss = L1_lambda * L1_norm \n",
    "        #L2 regularization\n",
    "        L2_norm = sum(p.pow(2).sum() for p in model.conv.parameters())\n",
    "        L2_loss = L2_lambda * L2_norm\n",
    "\n",
    "        loss = err_loss3 + err_loss2 + L1_loss + L2_loss #+ curv_loss\n",
    "\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #batch loss\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "\n",
    "    # Return the average training loss\n",
    "    err_loss2 = np.mean(err_losses2)\n",
    "    err_loss3 = np.mean(err_losses3)\n",
    "    # curv_loss = np.mean(curv_losses)\n",
    "    return err_loss2, err_loss3\n",
    "\n",
    "    # VALIDATION FUNCTION\n",
    "def val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device=device):\n",
    "    lane_keeper.eval()\n",
    "    err_losses3 = []\n",
    "    err_losses2 = []\n",
    "    # curv_losses = []\n",
    "    for (input, regr_label) in tqdm(val_dataloader):\n",
    "        input, regr_label =input.to(device), regr_label.to(device)\n",
    "        output = lane_keeper(input)\n",
    "\n",
    "        regr_out = output\n",
    "        err2 = regr_out[:, 0]\n",
    "        err3 = regr_out[:, 1]\n",
    "        # curv_out = regr_out[:, 2]\n",
    "\n",
    "        err2_label = regr_label[:, 0]\n",
    "        err3_label = regr_label[:, 1]\n",
    "        # curv_label = regr_label[:, 2]\n",
    "\n",
    "        err_loss3 = 1.0*regr_loss_fn(err3, err3_label)\n",
    "        err_loss2 = 1.0*regr_loss_fn(err2, err2_label)\n",
    "        # curv_loss = 1.0*regr_loss_fn(curv_out, curv_label)\n",
    "        loss = err_loss3 + err_loss2\n",
    "\n",
    "        err_losses2.append(err_loss2.detach().cpu().numpy())\n",
    "        err_losses3.append(err_loss3.detach().cpu().numpy())\n",
    "        # curv_losses.append(curv_loss.detach().cpu().numpy())\n",
    "    return np.mean(err_losses2), np.mean(err_losses3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/30,  loss = MSELoss() \n",
      "yaw_err_loss3: 0.0146,   Val: 0.0148\n",
      "lat_err_loss2: 0.0016,   Val: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 8/41 [00:04<00:18,  1.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42825/4122303408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mregr_loss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr_loss_fn1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mregr_loss_fn2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0merr_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mval_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_42825/1434882309.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loop over the training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Move the input and target data to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "#parameters\n",
    "lr = 0.001 #0.005\n",
    "epochs = 30\n",
    "#regularization is applied only to convolutional section, add weight decay to apply it to all layers\n",
    "L1_lambda = 1e-4 #9e-4\n",
    "L2_lambda = 1e-2 #1e-2\n",
    "optimizer = torch.optim.Adam(lane_keeper.parameters(), lr=lr, weight_decay=9e-5) #wd = 2e-3# 3e-5\n",
    "regr_loss_fn1 = nn.MSELoss() #before epochs/2\n",
    "regr_loss_fn2 = nn.MSELoss() #after epochs/2 for finetuning\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    try:\n",
    "    # if True:\n",
    "        regr_loss_fn = regr_loss_fn1 if epoch < epochs//2 else regr_loss_fn2\n",
    "        err_loss2, err_loss3 = train_epoch(lane_keeper, train_dataloader, regr_loss_fn, optimizer, L1_lambda, L2_lambda, device)\n",
    "        val_loss2, val_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "        clear_output(wait=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    print(f\"Epoch  {epoch+1}/{epochs},  loss = {regr_loss_fn} \\nyaw_err_loss3: {err_loss3:.4f},   Val: {val_loss3:.4f}\")\n",
    "    print(f\"lat_err_loss2: {err_loss2:.4f},   Val: {val_loss2:.4f}\")\n",
    "    # print(f\"curv_loss: {curv_loss}\")\n",
    "    torch.save(lane_keeper.state_dict(), model_name)\n",
    "\n",
    "#Note: sweet spot for training is around 0.016 -> 0.020, also note that training can get stuck, and loss can start improving randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD THE NETWORK\n",
    "lane_keeper.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17168/1116345086.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# EVALUATE ON TEST SET (UNSEEN DATA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlane_keeper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0merr_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_loss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlane_keeper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"lateral_err2_loss: {err_loss2}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# EVALUATE ON TEST SET (UNSEEN DATA)\n",
    "err_loss2, err_loss3 = val_epoch(lane_keeper, val_dataloader, regr_loss_fn, device)\n",
    "\n",
    "print(f\"lateral_err2_loss: {err_loss2}\")\n",
    "print(f\"yaw_err3_loss: {err_loss3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 5, 5)\n",
      "(4, 4, 5, 5)\n",
      "(128, 4, 5, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAByCAYAAADj/oBRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFuElEQVR4nO3avWtVexbH4XVMYgx5DwpGi1ERlDSClYiojdoECxs7CystBKuIEvC1sfAPsIyFom0EQRCCRsTSgNopF4tEByXmDZUczxS3Dtws1mXuDM/T7s1nH/Lj5MsOabRarQAA1mfDf/sDAMD/IgMKAAkGFAASDCgAJBhQAEhoX8/N3d3drYGBgbKH9/T0lLU6OjrKWhERb9++LWv19/eXtVZWVuLXr1+Nilaj0Sj9F+zBwcGy1q5du8paERFLS0tlrcXFxbLW/Px8LC8vl5xnRERPT09raGioKhednZ1lrbm5ubJWteHh4bLW58+f4/v37yVn2tXV1err66tIRUREs9ksa339+rWsFVG7B9u2bStrzc3NrXme6xrQgYGBOHfuXM2nioiDBw+WtSp/YBERIyMjZa1Dhw6Vtaanp8ta1Y4dO1bWevjwYVkrIuL58+dlrampqbLW3bt3y1oREUNDQzE2NlbW27FjR1nr9u3bZa1q4+PjZa0LFy6Utfr6+uL06dNlvYWFhbLWxMREWSsiYv/+/WWtq1evlrXOnz+/5jV/wgWABAMKAAkGFAASDCgAJBhQAEgwoACQYEABIMGAAkCCAQWABAMKAAkGFAASDCgAJBhQAEgwoACQYEABIMGAAkCCAQWAhPb13Lxhw4bo6uoqe/jMzExZa2RkpKwVEXHnzp2y1uTkZFmr2WyWtbq7u2Pfvn1lvdHR0bLWtWvXyloRERcvXixr3bhxo6y1uLhY1vo7rK6ulrWmp6fLWhERExMTZa0fP36UtX7//l3WWlpaipcvX5b1Kj/bzZs3y1oREePj42Wt9+/fl7U6OjrWvOYNFAASDCgAJBhQAEgwoACQYEABIMGAAkCCAQWABAMKAAkGFAASDCgAJBhQAEgwoACQYEABIMGAAkCCAQWABAMKAAkGFAASDCgAJLSv5+aVlZV48+ZN2cNHR0fLWk+fPi1rRUScOnWqrHXv3r2yVrPZLGs1Go3YuHFjWW/Tpk1lreHh4bJWRMTk5GRZa+/evWWtmZmZslZERG9vbxw5cqSsNzY2VtYaHBwsa0VEnDlzpqx19uzZstb8/HxZq62tLfr7+8t6169fL2tt3769rBURceXKlbLWnj17ylrLy8trXvMGCgAJBhQAEgwoACQYUABIMKAAkGBAASDBgAJAggEFgAQDCgAJBhQAEgwoACQYUABIMKAAkGBAASDBgAJAggEFgAQDCgAJBhQAEgwoACS0r+fm1dXV+PLlS9nDZ2dny1rNZrOsFRHRarXKWlu2bClrffjwoazV29sbR48eLet1dnaWtXp7e8taERGPHz8ua1WeZ6PRKGtFRLS1tcXg4GBZ79mzZ2WtR48elbUiIl68eFHWWlhYKGtV/i4aHh6O8fHxst7u3bvLWvfv3y9rRUQMDAyUtR48eFDW+vbt25rXvIECQIIBBYAEAwoACQYUABIMKAAkGFAASDCgAJBgQAEgwYACQIIBBYAEAwoACQYUABIMKAAkGFAASDCgAJBgQAEgwYACQIIBBYCE9vXc3Gw2Y3l5uezhr169KmudOHGirBURsXPnzrLW4cOHy1rv3r0ra0X8eaZVjh8/Xtaampoqa0VEfPr0qaz15MmTstbi4mJZKyJidnY2bt26VdbbvHlzWevkyZNlrYiIy5cvl7UOHDhQ1nr9+nVZq9Vqxerqalnv0qVLZa3K71RExNatW8taHz9+LGv9/PlzzWveQAEgwYACQIIBBYAEAwoACQYUABIMKAAkGFAASDCgAJBgQAEgwYACQIIBBYAEAwoACQYUABIMKAAkGFAASDCgAJBgQAEgwYACQIIBBYCERqvV+us3Nxr/jog//r6Pw1/wr1artaUi5Dz/EcrOM8KZ/kP4jv5/WfM81zWgAMCf/AkXABIMKAAkGFAASDCgAJBgQAEgwYACQIIBBYAEAwoACQYUABL+Aw5MGAaW3x6LAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAEhCAYAAAA9A2ZcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQxElEQVR4nO3dbWjV9f/H8ffZds7OrnWb82pzuqXisAwVL7KSoMQyEu+YQkgXRnWjG0kIGRRR0AVFNyIMbxQWhAiRWZZpQiZdm1ZgtVy2eVnb2ZXuzF24fX83ZCD/v5uf16H5e/Pr+bh7Xnvvu+/OXn5H+/SORVFkAOBN1n/7AgDgcignAC5RTgBcopwAuEQ5AXApRwknk8moqKgoON/T0yNdTFdXl5RXrsXMrLS0NDjb2tpq586diynzE4lElJeXJ12TIitL+7dkYGBAyieTSSnf0tKSiqJonPIx8Xg8Uj5Pfn6+dE3qf33u6+uT8sr1dHR0WHd3t/QeMjMrLi6OKioqgvOpVEqan0gkpLz6vsvOzg7OdnR0WDqdvuw9ksqpqKjIVq5cGZxvaGhQxtv+/ful/KJFi6T86tWrg7PPPfecNNvMLC8vz2644YbgfCymvW/V8jh37pyUnz59upTfvHlzk/QBdvFrmD9/fnB+zpw50vze3l4pf/LkSSk/b9684OyWLVuk2UMqKirslVdeCc6/9dZb0vzKykopr/6DW1xcHJx94403hn2NX+sAuEQ5AXCJcgLgEuUEwCXKCYBLlBMAlygnAC5RTgBcopwAuEQ5AXBJPVtns2bNCs7v3btXuphHH31Uyo8ZM0bK5+bmBmfVoyVmZufPn7cjR44E55WzfmZm1dXVUr6jo2NU52eit7fXfv/99+C8ck7LzGzixIlSPp1OS3nlvGim/5fZ3Nxcq62tDc6vX79emv/HH39IefVY2dGjR4OzbW1tw77GkxMAlygnAC5RTgBcopwAuEQ5AXCJcgLgEuUEwCXKCYBLlBMAlygnAC5RTgBcks7WxWIx6Xzam2++KV1MTU2NlD9x4sSozVd34pld3O+l3B8la2bSuT0z/ezhhAkTpHwm+vv77fTp08F5ZX+bmX4mcsGCBVJe2fmWyflMs4urmGbPnh2c3759uzRfPfOqnJUzu7jz8Z/AkxMAlygnAC5RTgBcopwAuEQ5AXCJcgLgEuUEwCXKCYBLlBMAlygnAC5RTgBcks7WRVFkFy5cCM6rZ+UaGxul/JQpU6R8f39/cDaTnWO9vb3W0NAQnL/11lul+SPt+Lqc8vJyKa/sJMxULBazZDIZnFfPH86cOVPKz5kzR8qfPHkyOJuVldm//X/99Zc9//zzwflPPvlEmv/TTz9JeeVn3sysrKwsODvSbkWenAC4RDkBcIlyAuAS5QTAJcoJgEuUEwCXKCcALlFOAFyinAC4RDkBcIlyAuCSdLausLDQFi9eHJyfNm2adDHqWSTlrJyZtosuOztbmj0kJyf8lqp7zdQ9dHV1dVJe3eGWiUQiYVVVVcH5/Px8aX5xcbGUX7t2rZTfuHFjcFY9kzYklUrZ1q1bg/Pq/kb1utSfy0y/7v/3ef+RKQDwD6OcALhEOQFwiXIC4BLlBMAlygmAS5QTAJcoJwAuUU4AXKKcALhEOQFwKabsZ4vFYi1m1jR6l+NKdRRF45QP+JfdHzPu0ZXI98eMezREKicAuFr4tQ6AS5QTAJcoJwAuUU4AXKKcALhEOQFwiXIC4BLlBMAlygmAS9JqqOzs7CgejwfnKysrpYvJzc2V8oODg1K+ubk5OJtOp62np0fa3VReXh5NnTo1OH/8+HFlvCUSCSlfUlIi5dVVVUeOHEmpxzPKy8ujKVOmBOc7Ojqka1LvkbLKy8wsmUwGZxsbGy2VSmk31cwSiUSkfB51ZZj6fW5vb5fyXV1dwdkoiiyKostekPSdicfj0s6xl156SRlvtbW1Uj6dTkv5zZs3B2d37dolzTYzmzp1qh08eDA4/8gjj0jza2pqpPzy5culvPqDWldXJ5//mjJlin3xxRfB+R07dkjz1XtUXl4u5WfMmBGcnT9/vjR7SDKZtEWLFgXn77zzTmm++n1+//33pfz+/fuDsyPtuOPXOgAuUU4AXKKcALhEOQFwiXIC4BLlBMAlygmAS5QTAJcoJwAuSX8q2tvbaw0NDcH5jRs3Shfz4IMPSvlVq1ZJ+WnTpgVn1aM0ZmYDAwPScYtrr71Wmv/CCy9I+QMHDkh59S+NM9HT02O//fZbcL61tVWa/8wzz0j5xYsXS/klS5YEZ1taWqTZQ6IosvPnzwfn1e/z3Llzpbz6Pv3ss8+k/HB4cgLgEuUEwCXKCYBLlBMAlygnAC5RTgBcopwAuEQ5AXCJcgLgEuUEwCXKCYBL0tm64uJi6WzRp59+Kl3Mvn37pPyyZcuk/KxZs4KzymqeId3d3Xbo0KHg/Jo1a6T56pqkp59+Wsp/+OGHUj4TiURCOuOorjHq6+uT8uoGn4cffljKZ6KwsNCWLl0anF+4cKE0Pz8/X8ofPnxYyisr4f7+++9hX+PJCYBLlBMAlygnAC5RTgBcopwAuEQ5AXCJcgLgEuUEwCXKCYBLlBMAlygnAC5JZ+tKSkps+fLlwfndu3dLF1NfXy/ld+7cKeXXrVsXnC0sLJRmm5m1tbXZtm3bgvMXLlyQ5is7A83Mbr/9dil/Nc7WNTU1SfsJN23aJM2/5ZZbpHxZWZmU/+qrr4Kz33zzjTR7SDqdtm+//TY4P2HCBGl+RUWFlB87dqyUV87Wtbe3D/saT04AXKKcALhEOQFwiXIC4BLlBMAlygmAS5QTAJcoJwAuUU4AXKKcALhEOQFwSTpb19bWZu+++25w/sknn5Qu5sUXX5Ty6tmlp556KjibSCSk2WZm8Xjcxo8fH5yfNGmSNP+JJ56Q8vfff7+UvxoKCgqkPWvl5eXS/NLSUik/0tmuy+ns7AzODgwMSLOHlJaW2tq1a4Pzs2fPluYvWLBAynd1dUn5lpaW4OyJEyeGfY0nJwAuUU4AXKKcALhEOQFwiXIC4BLlBMAlygmAS5QTAJcoJwAuUU4AXKKcALgUi6IoPByLtZhZ0+hdjivVURSNUz7gX3Z/zLhHVyLfHzPu0RCpnADgauHXOgAuUU4AXKKcALhEOQFwiXIC4BLlBMAlygmAS5QTAJcoJwAuUU4AXJL21sVisVE965KXlyfly8rKpLyyA+348ePW2toaU+aPHTs2UnfRKdra2qR8X1/fqM43s5R6dmzMmDHSPVK/hng8LuW7u7ulfGFhYXD29OnT1t7eLr2HzPSfs5wc6cdY3smYm5sr5ZXdjWfOnLGOjo7L3iPtqxJlZWkPZtOnT5fy9957r5R/4IEHgrNLly6VZptdXJKpLB2NxbT37fbt26X8sWPHpPy2bdukfBRF8uHUSZMm2TvvvBOcH2np4uVMmDBByh86dEjK33TTTcHZNWvWSLMvpRSO+o90ZWWllK+pqZHyGzZsCM7ed999w77Gr3UAXKKcALhEOQFwiXIC4BLlBMAlygmAS5QTAJcoJwAuUU4AXJL+QjyRSNjEiROD862trdLFpNNpKX/gwAEp/9hjjwVns7OzpdlmZlEU2YULF4LzLS0t0nzl+I2ZWVdXl5S/Gpt40um0/fDDD8H5zs5Oab56HEX9i/J169YFZ//8809p9qWU95F67Oi6666T8urxmDvuuCM4e/bs2WFf48kJgEuUEwCXKCcALlFOAFyinAC4RDkBcIlyAuAS5QTAJcoJgEuUEwCXKCcALsnbV5SNIQUFBdLsgYEBKX/8+HEpr5z1U842DYmiyAYHB4Pz6oqeu+66S8rv3r1byl8NZ8+etT179gTn33vvPWn+ihUrpPyzzz4r5V999dXg7EMPPSTNvpTyczZ27Fhp9q+//irle3t7pXx/f39wdqTznDw5AXCJcgLgEuUEwCXKCYBLlBMAlygnAC5RTgBcopwAuEQ5AXCJcgLgEuUEwCXpbF0sFpN2WFVXV0sXk0qlpLy60+z1118Pzqo75cwunpWbPHlycF7dN3bq1Ckpr+4ne/nll6X8448/LuXNLu7S+/LLL4Pz48aNk+Zv2LBByhcWFkr5qqqq4GxeXp40+1JZWeHPDerPjWrq1KlSfu7cucHZr7/+etjXeHIC4BLlBMAlygmAS5QTAJcoJwAuUU4AXKKcALhEOQFwiXIC4BLlBMAlygmAS/LeupH2TP1f9fX10uybb75Zyv/4449S/u233w7OKjvuLqXsG1M/x9KlS6W8Ol89C5nJ2bqSkhJbtmxZcH7VqlXSfHWHW3l5+ajNV86hXmr8+PG2bt264PzevXul+ep1TZw4Ucrv3LkzODt//vxhX+PJCYBLlBMAlygnAC5RTgBcopwAuEQ5AXCJcgLgEuUEwCXKCYBLlBMAlygnAC7FlLNysVisxcyaRu9yXKmOokhamvYvuz9m3KMrke+PGfdoiFROAHC18GsdAJcoJwAuUU4AXKKcALhEOQFwiXIC4BLlBMAlygmAS5QTAJcoJwAuSQusysrKoqqqquC8ejSms7NTyqfTaSmfSqWkfBRF4UvozCyZTEZFRUXB+cLCQul6uru7pXxzc7OULykpkfKdnZ2pDM7WSW8K9R6N9nGs8+fPB2cHBwfl95DZxXuUlTV6zw15eXlSXt1zd8011wRnGxsbLZVKXfYeSZ+1qqrK9u3bF5zv6+tTxtvHH38s5b/77jspv2XLFimvKioqspUrVwbn1SWZ33//vZR/7bXXpPyNN94o5Xft2jXqh1Ovv/56KT84ODiq+Z9//jk429PTI80ekpWVZfn5+VJeUVdXJ+XHjdPOLrNUE8D/NMoJgEuUEwCXKCcALlFOAFyinAC4RDkBcIlyAuAS5QTAJekvxAcGBqy9vT04/8EHH0gXc+rUKSmvXIuZ2fr164OzO3bskGabmRUUFNiiRYuC8+pxHeWv84euR1FTUyPlM1FWVmYrVqwIzldUVEjz1VMDiURCyqtHiDIxODhoXV1dwfna2lppfnFxsZQfP368lN+/f39w9ty5c8O+xpMTAJcoJwAuUU4AXKKcALhEOQFwiXIC4BLlBMAlygmAS5QTAJcoJwAuUU4AXJLO1uXm5kprX44ePSpdzLFjx6R8MpmU8pWVlcHZTFbzxONxmzx5cnC+qUlbXtLR0SHlV69eLeUPHz4s5TNRXFxst912W3D+888/l+bffffdUl45B2amnUtTzsddKpFISO9VddVTPB6X8so6LDOzbdu2BWdHOh/LkxMAlygnAC5RTgBcopwAuEQ5AXCJcgLgEuUEwCXKCYBLlBMAlygnAC5RTgBcks7W9fX12cmTJ4PzURRJF7NkyRIpv2fPHimvnPU7e/asNNvMLCcnx0pLS4PzW7dulebPmDFDyufn50v5+vp6KZ+J0tJSu+eee0ZtfmNjo5QfGBiQ8vPmzQvOHjx4UJp9KeW62trapNkzZ86U8n19fVJe2T850myenAC4RDkBcIlyAuAS5QTAJcoJgEuUEwCXKCcALlFOAFyinAC4RDkBcIlyAuCSdLaus7PTPvroo+C8euZn7dq1Uv6XX36R8jU1NcHZ5uZmabaZWUFBgS1cuDA4f+bMGWl+bm6ulN+3b5+UT6VSUj4T3d3ddujQoeB8bW2tNH/Tpk1SXtkzaKbtDuzv75dmD8nOzrYxY8YE5wsKCqT5OTnSj710LWba2bqR8OQEwCXKCYBLlBMAlygnAC5RTgBcopwAuEQ5AXCJcgLgEuUEwCXKCYBLlBMAl2LKbrlYLNZiZk2jdzmuVEdRNE75gH/Z/THjHl2JfH/MuEdDpHICgKuFX+sAuEQ5AXCJcgLgEuUEwCXKCYBLlBMAlygnAC5RTgBcopwAuPQf2E8JM9J8QPcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 360x360 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE CONVOLUTIONAL FILTERS\n",
    "conv_layers = []\n",
    "children = list(lane_keeper.children())\n",
    "for i in range(len(children)):\n",
    "    if isinstance(children[i], nn.Conv2d):\n",
    "        conv_layers.append(children[i])\n",
    "    elif isinstance(children[i], nn.Sequential):\n",
    "        for child in children[i].children():\n",
    "            if isinstance(child, nn.Conv2d):\n",
    "                conv_layers.append(child)\n",
    "\n",
    "c0 = conv_layers[0].weight.data.cpu().numpy()\n",
    "c1 = conv_layers[1].weight.data.cpu().numpy()\n",
    "c2 = conv_layers[2].weight.data.cpu().numpy()\n",
    "\n",
    "def plot_nchw_data(data, h_num, v_num, title, size=(10, 10)):\n",
    "    fig, axs = plt.subplots(h_num, v_num, figsize=size)\n",
    "    shape = data.shape\n",
    "    data = data.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if idx < len(data):\n",
    "            ax.imshow(data[idx,:,:], cmap='gray')\n",
    "    if title is not None:\n",
    "        plt.suptitle(title)\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.97], h_pad=0, w_pad=0)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# fig0 = plot_nchw_data(c0, 4, 4, 'conv0')\n",
    "print(c0.shape)\n",
    "print(c1.shape)\n",
    "print(c2.shape)\n",
    "\n",
    "fig0 = plot_nchw_data(c0, 1, 4, None, size=(8,2))\n",
    "\n",
    "fig1 = plot_nchw_data(c1, 4, 4, None, size=(5,5)) \n",
    "\n",
    "fig2 = plot_nchw_data(c2, 8, 8, None, size=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaneKeeper(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout(p=0.2, inplace=False)\n",
       "    (11): Conv2d(4, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERT TO ONNX MODEL FOR OPENCV\n",
    "lane_keeper.load_state_dict(torch.load(model_name))\n",
    "\n",
    "#save the model so that opencv can load it\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "\n",
    "device = torch.device('cpu')\n",
    "lane_keeper.to(device)\n",
    "\n",
    "# set the model to inference mode\n",
    "lane_keeper.eval()\n",
    "\n",
    "# Create some sample input in the shape this model expects \n",
    "# This is needed because the convertion forward pass the network once \n",
    "dummy_input = torch.randn(1, num_channels, SIZE[1], SIZE[0])\n",
    "torch.onnx.export(lane_keeper, dummy_input, onnx_lane_keeper_path, verbose=True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lane_keeper.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4000.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.01462647 0.04006106]]\n",
      "Predictions shape: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST WITH OPENCV\n",
    "sample_image = \"training_imgs/img_1.png\"\n",
    "images = [cv.imread(f\"training_imgs/img_{i+1}.png\") for i in range(100)]\n",
    " \n",
    "#The Magic:\n",
    "lk =  cv.dnn.readNetFromONNX(onnx_lane_keeper_path) \n",
    "\n",
    "avg_col = (0,0,0) if num_channels == 3 else 0\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    image = images[i]\n",
    "    image = cv.resize(image, SIZE)\n",
    "    if num_channels == 1:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    blob = cv.dnn.blobFromImage(image, 1.0, SIZE, avg_col, swapRB=True, crop=False)\n",
    "    # print(blob.shape)\n",
    "    lk.setInput(blob)\n",
    "    preds = lk.forward()\n",
    "    # print(f\"Predictions: {preds[0][2]}\")\n",
    "\n",
    "print(f\"Predictions: {preds}\")\n",
    "print(f\"Predictions shape: {preds.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee89b7c6bc96453738565335b56b694d8a30ac65e979633b683f8408c8233c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
